{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq-to-seq.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9wFwj0WmghBi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**This project is about using neural network to train a sequence to sequence model on a dataset of English and French**\n",
        "**sentences that can translate  new senetence from English to French**"
      ]
    },
    {
      "metadata": {
        "id": "VvMOQHXUh-iI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import the packages need, connect to gmail for authentication as colab is used to read the data**"
      ]
    },
    {
      "metadata": {
        "id": "VqJ59KcHgh2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gda0tDz6EEZJ",
        "colab_type": "code",
        "outputId": "34d5d40d-51c0-4626-c70d-9f5609ed43ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
            "  from pandas.core import datetools\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "MASXXnhCEIdO",
        "colab_type": "code",
        "outputId": "b4e409d8-5b5c-42b5-9535-2cc2659c17c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "54YimOsogoyj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, LSTM, Dense, Embedding, RepeatVector, TimeDistributed\n",
        "import numpy as np\n",
        "\n",
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from time import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gIKImQehEL8D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oi1GNM42E-cW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file1 = drive.CreateFile({'id':'1Fs_lfbowZAWrBFV4OAtpQI_S5LeibYdj'})\n",
        "file1.GetContentFile('Translation_Data.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fVh6rJLBFNHh",
        "colab_type": "code",
        "outputId": "da00fa1b-2190-4228-b39e-9ee2a8a2e531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  sample_data  Translation_Data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "d8Apsz8-FR2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip -qq Translation_Data.zip  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p3DcmeOZF5km",
        "colab_type": "code",
        "outputId": "12fe06e4-bce0-4606-817a-8db9c6a92651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  sample_data\t      small_vocab_fr.txt\n",
            "__MACOSX  small_vocab_en.txt  Translation_Data.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ytu4GYwhiMdh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Split the english and french document at new line character so that it can be each sentence**"
      ]
    },
    {
      "metadata": {
        "id": "1VHvt7WpGRxW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "eng_chars = []\n",
        "fra_chars = []\n",
        "\n",
        "\n",
        "eng_sent = open('small_vocab_en.txt', encoding='utf-8').read().split('\\n')\n",
        "fra_sent = open('small_vocab_fr.txt', encoding='utf-8').read().split('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UaT_lYzIGTn1",
        "colab_type": "code",
        "outputId": "bd121241-4e56-49cc-da42-cbe0e88276d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(eng_sent))\n",
        "print(len(fra_sent))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "137861\n",
            "137861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "93deySmGPl46",
        "colab_type": "code",
        "outputId": "3cd411c0-3aaa-4617-ccfa-a26f7627fcde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(eng_sent[0])\n",
        "print(fra_sent[0])\n",
        "\t\t\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zH5MTDLfRenW",
        "colab_type": "code",
        "outputId": "c082e1a4-6101-496f-fada-6691af51b3d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(eng_sent[1])\n",
        "print(fra_sent[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the united states is usually chilly during july , and it is usually freezing in november .\n",
            "les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8iltO5r5ipVf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Get each line and take the word and add to english and french dictionary**"
      ]
    },
    {
      "metadata": {
        "id": "fqlOKoTHQDWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nb_samples = len(fra_sent)\n",
        "\n",
        "for line in range(nb_samples):\n",
        "  eng_line = eng_sent[line]\n",
        "  fra_line = fra_sent[line]\n",
        "  \n",
        "  \n",
        "    \n",
        "\n",
        "  for ch in eng_line:\n",
        "        if (ch not in eng_chars):\n",
        "            eng_chars.append(ch)\n",
        "  for ch in fra_line:\n",
        "        if (ch not in fra_chars):\n",
        "            fra_chars.append(ch)\n",
        "        \n",
        "#fra_chars = sorted(list(fra_chars))\n",
        "#eng_chars = sorted(list(eng_chars))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xXqZvewPdruK",
        "colab_type": "code",
        "outputId": "e4ec75bb-07fe-4be1-87db-c04ed29f9961",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(eng_sent[0])\n",
        "print(fra_sent[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lWXUl8dQjAUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1) **create a dictionary to index each english character - key is index and value is english character**\n",
        "2)** create a dictionary to get english character given its index - key is english character and value is index**\n",
        "3) **create a dictionary to index each french character - key is index and value is french character**\n",
        "4)** create a dictionary to get french character given its index - key is french character and value is index**"
      ]
    },
    {
      "metadata": {
        "id": "5q6dgVGrf2SW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# dictionary to index each english character - key is index and value is english character\n",
        "eng_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get english character given its index - key is english character and value is index\n",
        "eng_char_to_index_dict = {}\n",
        "\n",
        "for k, v in enumerate(eng_chars):\n",
        "    eng_index_to_char_dict[k] = v\n",
        "    eng_char_to_index_dict[v] = k\n",
        "\n",
        "# dictionary to index each french character - key is index and value is french character\n",
        "fra_index_to_char_dict = {}\n",
        "\n",
        "# dictionary to get french character given its index - key is french character and value is index\n",
        "fra_char_to_index_dict = {}\n",
        "for k, v in enumerate(fra_chars):\n",
        "    fra_index_to_char_dict[k] = v\n",
        "    fra_char_to_index_dict[v] = k\n",
        "\n",
        "max_len_eng_sent = max([len(line) for line in eng_sent])\n",
        "max_len_fra_sent = max([len(line) for line in fra_sent])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-U-z5zv8f5Ll",
        "colab_type": "code",
        "outputId": "8b6aff43-0d23-4217-ea5b-83f4961ef028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(max_len_eng_sent)\n",
        "print(max_len_fra_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102\n",
            "114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eKHt-OsJnBL6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create a one hot encoding of Frecnch and English senetences**"
      ]
    },
    {
      "metadata": {
        "id": "kvodbG7Pf_Qv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenized_eng_sentences = np.zeros(shape=(nb_samples, max_len_eng_sent, len(eng_chars)), dtype='float32')\n",
        "tokenized_fra_sentences = np.zeros(shape=(nb_samples, max_len_fra_sent, len(fra_chars)), dtype='float32')\n",
        "target_data = np.zeros((nb_samples, max_len_fra_sent, len(fra_chars)), dtype='float32')\n",
        "\n",
        "# Vectorize the english and french sentences\n",
        "\n",
        "for i in range(nb_samples):\n",
        "    for k, ch in enumerate(eng_sent[i]):\n",
        "        tokenized_eng_sentences[i, k, eng_char_to_index_dict[ch]] = 1\n",
        "\n",
        "    for k, ch in enumerate(fra_sent[i]):\n",
        "        tokenized_fra_sentences[i, k, fra_char_to_index_dict[ch]] = 1\n",
        "\n",
        "        # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
        "        if k > 0:\n",
        "            target_data[i, k - 1, fra_char_to_index_dict[ch]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3HcLFCyvnOOo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create a Encode for the english  word and Decode for the french word**"
      ]
    },
    {
      "metadata": {
        "id": "iZCcj-6qgWoc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Encoder model\n",
        "encoder_input = Input(shape=(None, len(eng_chars)))\n",
        "encoder_LSTM = LSTM(256, return_state=True)\n",
        "encoder_outputs, encoder_h, encoder_c = encoder_LSTM(encoder_input)\n",
        "encoder_states = [encoder_h, encoder_c]\n",
        "\n",
        "# Decoder model\n",
        "decoder_input = Input(shape=(None, len(fra_chars)))\n",
        "decoder_LSTM = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_out, _, _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(fra_chars), activation='softmax')\n",
        "decoder_out = decoder_dense(decoder_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YWRZvkLFnjSF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Run the model**"
      ]
    },
    {
      "metadata": {
        "id": "1c3BoLvIg0k0",
        "colab_type": "code",
        "outputId": "42fbf534-9af7-4016-9e34-90e6bad3b037",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1314
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[decoder_out])\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "\n",
        "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
        "\n",
        "model.fit(x=[tokenized_eng_sentences, tokenized_fra_sentences],\n",
        "          y=target_data,\n",
        "          batch_size=64,\n",
        "          epochs=40,\n",
        "          validation_split=0.2, verbose=1, callbacks=[tensorboard])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 110288 samples, validate on 27573 samples\n",
            "Epoch 1/40\n",
            "110288/110288 [==============================] - 721s 7ms/step - loss: 0.3996 - val_loss: 0.1486\n",
            "Epoch 2/40\n",
            "110288/110288 [==============================] - 715s 6ms/step - loss: 0.1204 - val_loss: 0.1145\n",
            "Epoch 3/40\n",
            "110288/110288 [==============================] - 714s 6ms/step - loss: 0.1062 - val_loss: 0.1101\n",
            "Epoch 4/40\n",
            "110288/110288 [==============================] - 686s 6ms/step - loss: 0.0984 - val_loss: 0.1007\n",
            "Epoch 5/40\n",
            "110288/110288 [==============================] - 693s 6ms/step - loss: 0.0933 - val_loss: 0.0936\n",
            "Epoch 6/40\n",
            "110288/110288 [==============================] - 686s 6ms/step - loss: 0.0887 - val_loss: 0.0871\n",
            "Epoch 7/40\n",
            "110288/110288 [==============================] - 683s 6ms/step - loss: 0.0845 - val_loss: 0.0868\n",
            "Epoch 8/40\n",
            "110288/110288 [==============================] - 682s 6ms/step - loss: 0.0807 - val_loss: 0.0832\n",
            "Epoch 9/40\n",
            "110288/110288 [==============================] - 699s 6ms/step - loss: 0.0778 - val_loss: 0.0781\n",
            "Epoch 10/40\n",
            "110288/110288 [==============================] - 692s 6ms/step - loss: 0.0756 - val_loss: 0.0765\n",
            "Epoch 11/40\n",
            "110288/110288 [==============================] - 719s 7ms/step - loss: 0.0739 - val_loss: 0.0750\n",
            "Epoch 12/40\n",
            "110288/110288 [==============================] - 734s 7ms/step - loss: 0.0723 - val_loss: 0.0727\n",
            "Epoch 13/40\n",
            "110288/110288 [==============================] - 727s 7ms/step - loss: 0.0706 - val_loss: 0.0714\n",
            "Epoch 14/40\n",
            "110288/110288 [==============================] - 742s 7ms/step - loss: 0.0687 - val_loss: 0.0697\n",
            "Epoch 15/40\n",
            "110288/110288 [==============================] - 745s 7ms/step - loss: 0.0659 - val_loss: 0.0657\n",
            "Epoch 16/40\n",
            "110288/110288 [==============================] - 734s 7ms/step - loss: 0.0612 - val_loss: 0.0620\n",
            "Epoch 17/40\n",
            "110288/110288 [==============================] - 725s 7ms/step - loss: 0.0582 - val_loss: 0.0591\n",
            "Epoch 18/40\n",
            "110288/110288 [==============================] - 738s 7ms/step - loss: 0.0556 - val_loss: 0.0570\n",
            "Epoch 19/40\n",
            "110288/110288 [==============================] - 728s 7ms/step - loss: 0.0532 - val_loss: 0.0557\n",
            "Epoch 20/40\n",
            "110288/110288 [==============================] - 722s 7ms/step - loss: 0.0514 - val_loss: 0.0527\n",
            "Epoch 21/40\n",
            "110288/110288 [==============================] - 685s 6ms/step - loss: 0.0500 - val_loss: 0.0521\n",
            "Epoch 22/40\n",
            "110288/110288 [==============================] - 655s 6ms/step - loss: 0.0488 - val_loss: 0.0553\n",
            "Epoch 23/40\n",
            "110288/110288 [==============================] - 658s 6ms/step - loss: 0.0476 - val_loss: 0.0522\n",
            "Epoch 24/40\n",
            "110288/110288 [==============================] - 664s 6ms/step - loss: 0.0465 - val_loss: 0.0491\n",
            "Epoch 25/40\n",
            "110288/110288 [==============================] - 689s 6ms/step - loss: 0.0457 - val_loss: 0.0464\n",
            "Epoch 26/40\n",
            "110288/110288 [==============================] - 709s 6ms/step - loss: 0.0447 - val_loss: 0.0479\n",
            "Epoch 27/40\n",
            "110288/110288 [==============================] - 700s 6ms/step - loss: 0.0436 - val_loss: 0.0451\n",
            "Epoch 28/40\n",
            "110288/110288 [==============================] - 660s 6ms/step - loss: 0.0425 - val_loss: 0.0438\n",
            "Epoch 29/40\n",
            "110288/110288 [==============================] - 661s 6ms/step - loss: 0.0414 - val_loss: 0.0421\n",
            "Epoch 30/40\n",
            "110288/110288 [==============================] - 661s 6ms/step - loss: 0.0406 - val_loss: 0.0430\n",
            "Epoch 31/40\n",
            "110288/110288 [==============================] - 662s 6ms/step - loss: 0.0399 - val_loss: 0.0431\n",
            "Epoch 32/40\n",
            "110288/110288 [==============================] - 673s 6ms/step - loss: 0.0394 - val_loss: 0.0406\n",
            "Epoch 33/40\n",
            "110288/110288 [==============================] - 680s 6ms/step - loss: 0.0389 - val_loss: 0.0403\n",
            "Epoch 34/40\n",
            "110288/110288 [==============================] - 689s 6ms/step - loss: 0.0386 - val_loss: 0.0401\n",
            "Epoch 35/40\n",
            "110288/110288 [==============================] - 691s 6ms/step - loss: 0.0383 - val_loss: 0.0393\n",
            "Epoch 36/40\n",
            " 48960/110288 [============>.................] - ETA: 5:58 - loss: 0.0378Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srU9X3Ounr_x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create an interface to take the values from the encode to feed into the decode**"
      ]
    },
    {
      "metadata": {
        "id": "W5s6d3vuqSGw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Inference models for testing\n",
        "\n",
        "# Encoder inference model\n",
        "encoder_model_inf = Model(encoder_input, encoder_states)\n",
        "\n",
        "# Decoder inference model\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input,\n",
        "                                                 initial_state=decoder_input_states)\n",
        "\n",
        "decoder_states = [decoder_h , decoder_c]\n",
        "\n",
        "decoder_out = decoder_dense(decoder_out)\n",
        "\n",
        "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
        "                          outputs=[decoder_out] + decoder_states )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ybe0bcXtn3xK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Take each work of English and translate into French**"
      ]
    },
    {
      "metadata": {
        "id": "gqXpzboXqVd2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def decode_seq(inp_seq):\n",
        "    # Initial states value is coming from the encoder\n",
        "    states_val = encoder_model_inf.predict(inp_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "   # target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
        "\n",
        "    translated_sent = ''\n",
        "    stop_condition = False\n",
        "\n",
        "    while not stop_condition:\n",
        "\n",
        "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
        "\n",
        "        max_val_index = np.argmax(decoder_out[0, -1, :])\n",
        "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
        "        translated_sent += sampled_fra_char\n",
        "\n",
        "        if ((sampled_fra_char == '\\n') or (len(translated_sent) > max_len_fra_sent)):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1, 1, len(fra_chars)))\n",
        "        target_seq[0, 0, max_val_index] = 1\n",
        "\n",
        "        states_val = [decoder_h, decoder_c]\n",
        "\n",
        "    return translated_sent\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qmsSRARPqcYx",
        "colab_type": "code",
        "outputId": "194956ad-d3b1-462c-fc50-3d98906405a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "cell_type": "code",
      "source": [
        "for seq_index in range(10):\n",
        "    inp_seq = tokenized_eng_sentences[seq_index:seq_index+1]\n",
        "    translated_sent = decode_seq(inp_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', eng_sent[seq_index])\n",
        "    print('Decoded sentence:', translated_sent)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
            "Decoded sentence: anjeralemest le fruit merns lamplemons , mais la poire elle est moins aimé...mais .oine .'oir .a nour .....ét..s .a\n",
            "-\n",
            "Input sentence: the united states is usually chilly during july , and it is usually freezing in november .\n",
            "Decoded sentence: es états-unis est généralement froid en juillet , et il est jamais chaud en avril .t .ais il est parfois froid en j\n",
            "-\n",
            "Input sentence: california is usually quiet during march , and it is usually hot in june .\n",
            "Decoded sentence: a fruit est généralement calme en mars , et il est parfois froid en juin .t .ais il est parfois chaud en juillet ..\n",
            "-\n",
            "Input sentence: the united states is sometimes mild during june , and it is cold in september .\n",
            "Decoded sentence: es - rons est parfois froid en juin , et il est parfois chaud en juillet ..t en été est jamais chaud .a printemps .\n",
            "-\n",
            "Input sentence: your least liked fruit is the grape , but my least liked is the apple .\n",
            "Decoded sentence: ouc, moins aimé fruit est le raisin , mais mon moins aimé est la pomme ..mais et les citrons verts .t les .ranses .\n",
            "-\n",
            "Input sentence: his favorite fruit is the orange , but my favorite is the grape .\n",
            "Decoded sentence: otre fruit préféré est l'orange , mais mon préféré est le raisin ..raine .st l' étaber .t .s ne gèus ..mis .out .ai\n",
            "-\n",
            "Input sentence: paris is relaxing during december , but it is usually chilly in july .\n",
            "Decoded sentence: es étatseunons sont nos fruits les plus aimés , mais le citron est mon moins aimé .st la chaux ..mais .otré .ouns .\n",
            "-\n",
            "Input sentence: new jersey is busy during spring , and it is never hot in march .\n",
            "Decoded sentence: ancfnine est humide au printemps , et il est jamais chaud en juillet .re pours .amais .ochaver ..ini .rovelles .a c\n",
            "-\n",
            "Input sentence: our least liked fruit is the lemon , but my least liked is the grape .\n",
            "Decoded sentence: llpense est mon auit le plus aimé .ouri .oine préféré .sin le raisins .out le .ausone .out .nicil'o.s ..uila .ours \n",
            "-\n",
            "Input sentence: the united states is sometimes busy during january , and it is sometimes warm in november .\n",
            "Decoded sentence: es fruits est parfois froid en janvier , et il est parfois froid en juin .t .ais il est parfois chaud en juillet ..\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}